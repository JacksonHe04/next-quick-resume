/**
 * æµ‹è¯•AIé…ç½®å’Œè¿æ¥
 */

async function testAIConnection() {
  try {
    console.log('ğŸ” æµ‹è¯•AIæœåŠ¡è¿æ¥...')
    
    const testData = {
      model: 'glm-4.5-flash',
      messages: [
        {
          role: 'user',
          content: 'ä½ å¥½ï¼Œè¯·å›å¤"è¿æ¥æˆåŠŸ"'
        }
      ],
      max_tokens: 50,
      temperature: 0.7
    }

    const response = await fetch('https://open.bigmodel.cn/api/paas/v4/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': 'Bearer fb006984119d45edbe544bc2f84fadcb.SIwkNh5LrS1tckHA'
      },
      body: JSON.stringify(testData)
    })

    console.log('ğŸ“¡ å“åº”çŠ¶æ€:', response.status)
    console.log('ğŸ“¡ å“åº”å¤´:', Object.fromEntries(response.headers.entries()))
    
    const result = await response.text()
    console.log('ğŸ“„ å“åº”å†…å®¹:', result)

    if (response.ok) {
      console.log('âœ… AIæœåŠ¡è¿æ¥æˆåŠŸ!')
    } else {
      console.log('âŒ AIæœåŠ¡è¿æ¥å¤±è´¥')
    }

  } catch (error) {
    console.error('ğŸ’¥ è¿æ¥æµ‹è¯•å¼‚å¸¸:', error.message)
  }
}

// è¿è¡Œæµ‹è¯•
testAIConnection()